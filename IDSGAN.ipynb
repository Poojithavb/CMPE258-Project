{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDSGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1dgshOPJWRGAg2Dhr99T9BbzpWxWKU934",
      "authorship_tag": "ABX9TyPWbNH5M4GRhz9l3NjLevNb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Poojithavb/CMPE258-Project/blob/master/IDSGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqFrcKU9l2n-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "59d6bb81-0f5c-41c8-a007-6a3f4c42d575"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVSStAKhfVzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") \n",
        "import sys\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT_GHH3EgKV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pwd = os.getcwd()\n",
        "path = pwd + '/drive/My Drive/MachineLearningCVE'\n",
        "dfs = []\n",
        "filenames = glob(path + \"/*.csv\")\n",
        "filenames.sort()\n",
        "for filename in filenames:\n",
        "    dfs.append(pd.read_csv(filename))\n",
        "    \n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "df['Flow Bytes/s'] = df['Flow Bytes/s'].astype(float)\n",
        "df[' Flow Packets/s'] = df[' Flow Packets/s'].astype(float)\n",
        "df = df.fillna(0)\n",
        "df = df.replace([np.inf, -np.inf], sys.maxsize)\n",
        "#print(\"Columns which have NaN or NULLS: \")\n",
        "#for col in df.columns:\n",
        "#    print('%s: %d' % (col, df[col].isna().sum()))\n",
        "substr = 'ï¿½'\n",
        "df[' Label'] = df[' Label'].str.replace(substr, '-')\n",
        "encoding = {\n",
        "        \" Label\": {\"BENIGN\": 0, \"FTP-Patator\": 1, \"SSH-Patator\": 1, \n",
        "                   \"DoS slowloris\": 1, \"DoS Slowhttptest\": 1, \"DoS Hulk\": 1, \n",
        "                   \"DoS GoldenEye\": 1, \"Heartbleed\": 1, \n",
        "                   \"Web Attack - Brute Force\" : 1, \"Web Attack - XSS\": 1,\n",
        "                   \"Web Attack - Sql Injection\": 1, \"Infiltration\" : 1, \n",
        "                   \"Bot\":1, \"DDoS\":1,\"PortScan\": 1}\n",
        "        }\n",
        "df.replace(encoding, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7IoAtWThJDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_data = df.drop(columns=[\n",
        "    \"Fwd Avg Bytes/Bulk\",              \n",
        "    \" Fwd Avg Packets/Bulk\",           \n",
        "    \" Fwd Avg Bulk Rate\",              \n",
        "    \" Bwd Avg Bytes/Bulk\",             \n",
        "    \" Bwd Avg Packets/Bulk\", \n",
        "    \"Bwd Avg Bulk Rate\", \n",
        "    \" Bwd PSH Flags\", \n",
        "    \" Bwd URG Flags\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maxQLtenVdyJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "a74542e8-0541-4a4d-dd0f-3106eaa9367e"
      },
      "source": [
        "full_data.columns"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([' Destination Port', ' Flow Duration', ' Total Fwd Packets',\n",
              "       ' Total Backward Packets', 'Total Length of Fwd Packets',\n",
              "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max',\n",
              "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean',\n",
              "       ' Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
              "       ' Bwd Packet Length Min', ' Bwd Packet Length Mean',\n",
              "       ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s',\n",
              "       ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min',\n",
              "       'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max',\n",
              "       ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std',\n",
              "       ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Fwd URG Flags',\n",
              "       ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s',\n",
              "       ' Bwd Packets/s', ' Min Packet Length', ' Max Packet Length',\n",
              "       ' Packet Length Mean', ' Packet Length Std', ' Packet Length Variance',\n",
              "       'FIN Flag Count', ' SYN Flag Count', ' RST Flag Count',\n",
              "       ' PSH Flag Count', ' ACK Flag Count', ' URG Flag Count',\n",
              "       ' CWE Flag Count', ' ECE Flag Count', ' Down/Up Ratio',\n",
              "       ' Average Packet Size', ' Avg Fwd Segment Size',\n",
              "       ' Avg Bwd Segment Size', ' Fwd Header Length.1', 'Subflow Fwd Packets',\n",
              "       ' Subflow Fwd Bytes', ' Subflow Bwd Packets', ' Subflow Bwd Bytes',\n",
              "       'Init_Win_bytes_forward', ' Init_Win_bytes_backward',\n",
              "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean',\n",
              "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
              "       ' Idle Max', ' Idle Min', ' Label'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSU4fQ0thM5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "functional_cols = ['Idle Mean',\n",
        " ' Idle Std',\n",
        " ' Idle Min',\n",
        " ' Idle Max',\n",
        " ' Avg Bwd Segment Size',\n",
        " ' Average Packet Size',\n",
        " ' URG Flag Count',\n",
        " 'FIN Flag Count',\n",
        " ' Packet Length Variance',\n",
        " ' Packet Length Std',\n",
        " ' Packet Length Mean',\n",
        " ' Max Packet Length',\n",
        " ' Min Packet Length',\n",
        " ' Fwd IAT Max',\n",
        " ' Fwd IAT Std',\n",
        " ' Fwd IAT Mean',\n",
        " 'Fwd IAT Total',\n",
        " ' Flow IAT Max',\n",
        " ' Flow IAT Mean',\n",
        " ' Flow IAT Std',\n",
        " ' Bwd Packet Length Std',\n",
        " 'Bwd Packet Length Max',\n",
        " ' Bwd Packet Length Min',\n",
        " ' Bwd Packet Length Mean',\n",
        " ' Flow Duration']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXLpyI41hSQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "functional_features = np.array([col in functional_cols for col in full_data.columns[:-1]], dtype=np.float64)\n",
        "nonFunctional_features = np.array([col not in functional_cols for col in full_data.columns[:-1]], dtype=np.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1ceZTpUhj0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _toarray(X, Y):\n",
        "    X = X.astype(float)\n",
        "    X = np.asarray(X)\n",
        "    Y = np.asarray(Y)\n",
        "    #Y = to_categorical(Y, 2)\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X)\n",
        "    X = scaler.transform(X)\n",
        "    return X, Y\n",
        "    \n",
        "def _getData(df):\n",
        "    _all = df\n",
        "    _norm = df.loc[df[' Label'] == 0]\n",
        "    _mal = df.loc[df[' Label'] != 0]\n",
        "    l_all = _all[' Label'][:,None]\n",
        "    l_norm = _norm[' Label']\n",
        "    l_mal = _mal[' Label']\n",
        "    _all = _all.drop([' Label'], axis=1)\n",
        "    _mal = _mal.drop([' Label'], axis=1)\n",
        "    _norm = _norm.drop([' Label'], axis=1)\n",
        "    data_all, label_all = _toarray(_all, l_all)\n",
        "    data_mal, label_mal = _toarray(_mal, l_mal)\n",
        "    data_norm, label_norm = _toarray(_norm, l_norm)\n",
        "    return data_all, label_all, data_mal, label_mal, data_norm, label_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWX7yc2Eho_2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7794efe3-52e2-480c-ff81-70db87401daa"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_res, y_res = rus.fit_resample(full_data.iloc[:,:-1], full_data.iloc[:,-1])\n",
        "print('Resampled dataset shape %s' % Counter(y_res))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resampled dataset shape Counter({0: 557646, 1: 557646})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EpM4iFNiDVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataframe = pd.DataFrame(X_res)\n",
        "dataframe[' Label'] = y_res "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNy_rE99iLtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_all, Y_all, malicious_data, Y_mal, normal_data, Y_norm = _getData(dataframe)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nai1YhciOOo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "354b84e5-4c25-4937-faca-409f1b4d8ead"
      },
      "source": [
        "print(\"Malicious_data\",malicious_data.shape)\n",
        "print(\"Normal_data\",normal_data.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Malicious_data (557646, 70)\n",
            "Normal_data (557646, 70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9myWtqEqiQjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train,y_test = train_test_split(X_all,Y_all,test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7ON_F-yiXwW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "a9b2f7d4-6ab5-40fa-dd10-1e592aaa56c6"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
              "                     criterion='gini', max_depth=None, max_features='auto',\n",
              "                     max_leaf_nodes=None, max_samples=None,\n",
              "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                     min_samples_leaf=1, min_samples_split=2,\n",
              "                     min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                     n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
              "                     warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XZnmIeHiaDK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a45894e2-1890-42a2-b8dc-b4c955f3acb4"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"F1 Score: \", f1_score(y_test, y_pred, average='micro'))\n",
        "print(\"MSE: \", mean_squared_error(y_test, y_pred))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 Score:  0.9984578140410652\n",
            "MSE:  0.0015421859589348158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feCUmxCgiqjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _generator(input=(70,)):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(32, input_shape=(70,)))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    #model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(64))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    #model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(128))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    #model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(256))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    #model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(np.prod(70,)))\n",
        "    #model.add(layers.Activation('sigmoid'))\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HybE1Jfiit6L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "7d205063-f1f5-48b6-9cf4-24047bdac085"
      },
      "source": [
        "g = _generator()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32)                2272      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 70)                17990     \n",
            "=================================================================\n",
            "Total params: 65,638\n",
            "Trainable params: 64,678\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCpW0IPCiweu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _discriminator():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(128, input_shape=(70, )))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(128))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(64))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(1))\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quU3VaJ5iyWN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "a7e45ee2-7405-47cd-cc33-3bc485e35125"
      },
      "source": [
        "d = _discriminator()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 128)               9088      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 35,201\n",
            "Trainable params: 34,561\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvKhI5v7i1dU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(data, batch_size):\n",
        "    indices = np.random.randint(low=0, high=len(data), size=batch_size)\n",
        "    np.random.shuffle(indices)\n",
        "    batch_data = data[indices]\n",
        "    return batch_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD2Fn_Tvi3xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_latent_points(latent_dim, n):\n",
        "    x_input = np.random.uniform(0, 1, (latent_dim * n))\n",
        "    x_input = x_input.reshape(n, latent_dim)\n",
        "    return x_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9__Dvosi88a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "def generator_loss(outputs):\n",
        "    return cross_entropy(tf.ones_like(outputs), outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7xSzJvRi6Z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(ids_predicts, d_predicts):    \n",
        "    return cross_entropy(ids_predicts, d_predicts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc-Dj45MjCR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ids_compute(ids_classifer, ids_samples, batch_size):\n",
        "    a = tf.make_tensor_proto(ids_samples)\n",
        "    ids_samples = tf.make_ndarray(a)\n",
        "    ids_preds = ids_classifer.predict(ids_samples)\n",
        "    mse = mean_squared_error(np.hstack([np.ones(int(len(ids_preds)/2)), np.zeros(int(len(ids_preds)/2))]), ids_preds)\n",
        "    ids_predictions = tf.convert_to_tensor(ids_preds, float)\n",
        "    ids_samples = tf.convert_to_tensor(ids_samples, float)\n",
        "\n",
        "    return ids_samples, ids_predictions, mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W_rnoltqPEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _setTheshold(arr):\n",
        "  a = tf.where(tf.less(arr, 0.0), 0, arr) \n",
        "  arr = tf.where(tf.greater(a, 1.0), 1, a)\n",
        "  \n",
        "  return arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-PC0OD8iQYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_loss(losses):\n",
        "    labels = ['Discriminator', 'Generator', 'IDS-Classifier']\n",
        "    losses = np.array(losses)    \n",
        "    \n",
        "    fig, ax = plt.subplots()\n",
        "    plt.plot(losses.T[0], label='Discriminator')\n",
        "    plt.plot(losses.T[1], label='Generator')\n",
        "    plt.plot(losses.T[2], label='IDS-Classifier')\n",
        "    plt.title(\"Training Losses\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ici-CryFjI5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs=50, batch_size=10000, functional_features=functional_features,\n",
        "          nonFunctional_features=nonFunctional_features):\n",
        "    \n",
        "    losses = []\n",
        "    ids, generator, discriminator = clf, g, d\n",
        "    noise = generate_latent_points(70, batch_size)\n",
        "\n",
        "    generator_optimizer = tf.keras.optimizers.RMSprop(1e-3) #change to -4 \n",
        "    discriminator_optimizer = tf.keras.optimizers.RMSprop(1e-4) #change to \n",
        "    \n",
        "    for e in range(epochs):\n",
        "        for i in range(int(normal_data.shape[0]/batch_size)):\n",
        "            normal_samples = get_batch(normal_data, batch_size=batch_size)\n",
        "            malicious_samples = get_batch(malicious_data, batch_size=batch_size)\n",
        "            generator_in = malicious_samples + noise\n",
        "                        \n",
        "            normal_samples = tf.convert_to_tensor(normal_samples, float)\n",
        "            malicious_samples = tf.convert_to_tensor(malicious_samples, float)\n",
        "            functional_features = tf.convert_to_tensor(functional_features, float)\n",
        "            nonFunctional_features = tf.convert_to_tensor(nonFunctional_features, float)\n",
        "            generator_in = tf.convert_to_tensor(generator_in, float)\n",
        "\n",
        "\n",
        "            \n",
        "            with tf.GradientTape() as gTape, tf.GradientTape() as dTape:\n",
        "                generated_maliciousSamples = tf.math.add(tf.math.multiply(malicious_samples, \n",
        "                                                                          functional_features), \n",
        "                                                         _setTheshold(tf.math.multiply(generator(generator_in), \n",
        "                                                                     nonFunctional_features)))\n",
        "                \n",
        "                #print(generated_maliciousSamples)\n",
        "            \n",
        "                ids_samples = tf.concat([generated_maliciousSamples, normal_samples], 0)\n",
        "#                 ids_samples = tf.Session().run(ids_samples)\n",
        "#                 ids_predictions = ids.predict(ids_samples)\n",
        "                ids_samples, ids_predictions, ids_loss = ids_compute(ids, ids_samples, batch_size=batch_size)\n",
        "                #print(\"ids loss:\", ids_loss)\n",
        "                     \n",
        "                d_predictions = discriminator(ids_samples)\n",
        "            \n",
        "                d_loss = discriminator_loss(ids_predictions, d_predictions)*10\n",
        "                #print(\"dloss:\", d_loss)\n",
        "\n",
        "\n",
        "                disGenOut = discriminator(generated_maliciousSamples)\n",
        "            \n",
        "                g_loss = generator_loss(disGenOut)\n",
        "                #print(\"gloss:\", g_loss)\n",
        "    \n",
        "            gGradients = gTape.gradient(g_loss, generator.trainable_variables)\n",
        "            dGradients = dTape.gradient(d_loss, discriminator.trainable_variables)\n",
        "\n",
        "            generator_optimizer.apply_gradients(zip(gGradients, generator.trainable_variables))\n",
        "            discriminator_optimizer.apply_gradients(zip(dGradients, discriminator.trainable_variables))\n",
        "            losses.append((d_loss, g_loss, ids_loss))\n",
        "\n",
        "        print(\"Epoch:{:>3}/{} Discriminator Loss:{:>7.4f} Generator Loss:{:>7.4f} IDS Loss:{:>7.4f}\".format(\n",
        "            e+1, epochs, d_loss, g_loss, ids_loss)) \n",
        "    \n",
        "    plot_loss(losses)\n",
        "\n",
        "    return generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TamaBnd3nCxO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "e2c1b6a8-628f-4175-aa08-f27ac7aced5b"
      },
      "source": [
        "train()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1/50 Discriminator Loss: 3.8040 Generator Loss: 1.0099 IDS Loss: 0.4785\n",
            "Epoch:  2/50 Discriminator Loss: 2.4928 Generator Loss: 1.2715 IDS Loss: 0.4741\n",
            "Epoch:  3/50 Discriminator Loss: 2.1700 Generator Loss: 1.7197 IDS Loss: 0.4375\n",
            "Epoch:  4/50 Discriminator Loss: 1.2135 Generator Loss: 2.0638 IDS Loss: 0.4608\n",
            "Epoch:  5/50 Discriminator Loss: 1.2558 Generator Loss: 2.1373 IDS Loss: 0.3948\n",
            "Epoch:  6/50 Discriminator Loss: 1.4646 Generator Loss: 2.8431 IDS Loss: 0.3911\n",
            "Epoch:  7/50 Discriminator Loss: 0.9798 Generator Loss: 2.8839 IDS Loss: 0.3972\n",
            "Epoch:  8/50 Discriminator Loss: 1.1356 Generator Loss: 3.3782 IDS Loss: 0.4303\n",
            "Epoch:  9/50 Discriminator Loss: 0.9628 Generator Loss: 3.6804 IDS Loss: 0.4118\n",
            "Epoch: 10/50 Discriminator Loss: 0.8154 Generator Loss: 4.3445 IDS Loss: 0.3997\n",
            "Epoch: 11/50 Discriminator Loss: 0.8873 Generator Loss: 4.4316 IDS Loss: 0.3992\n",
            "Epoch: 12/50 Discriminator Loss: 0.8964 Generator Loss: 4.7323 IDS Loss: 0.3948\n",
            "Epoch: 13/50 Discriminator Loss: 0.7712 Generator Loss: 4.8373 IDS Loss: 0.3937\n",
            "Epoch: 14/50 Discriminator Loss: 5.7236 Generator Loss: 2.6976 IDS Loss: 0.3555\n",
            "Epoch: 15/50 Discriminator Loss: 2.2729 Generator Loss: 1.9273 IDS Loss: 0.3636\n",
            "Epoch: 16/50 Discriminator Loss: 1.7689 Generator Loss: 3.1329 IDS Loss: 0.3610\n",
            "Epoch: 17/50 Discriminator Loss: 1.6852 Generator Loss: 5.2922 IDS Loss: 0.3639\n",
            "Epoch: 18/50 Discriminator Loss: 1.5840 Generator Loss: 7.7034 IDS Loss: 0.3565\n",
            "Epoch: 19/50 Discriminator Loss: 1.5540 Generator Loss:10.6312 IDS Loss: 0.3626\n",
            "Epoch: 20/50 Discriminator Loss: 1.5141 Generator Loss:13.1582 IDS Loss: 0.3564\n",
            "Epoch: 21/50 Discriminator Loss: 1.4785 Generator Loss:16.0201 IDS Loss: 0.3647\n",
            "Epoch: 22/50 Discriminator Loss: 1.4900 Generator Loss:19.1032 IDS Loss: 0.3618\n",
            "Epoch: 23/50 Discriminator Loss: 1.4312 Generator Loss:20.7520 IDS Loss: 0.3554\n",
            "Epoch: 24/50 Discriminator Loss: 1.5531 Generator Loss:21.4688 IDS Loss: 0.3587\n",
            "Epoch: 25/50 Discriminator Loss: 1.5100 Generator Loss:20.8479 IDS Loss: 0.3634\n",
            "Epoch: 26/50 Discriminator Loss: 1.6657 Generator Loss:16.6518 IDS Loss: 0.3714\n",
            "Epoch: 27/50 Discriminator Loss: 1.6086 Generator Loss:14.4659 IDS Loss: 0.3728\n",
            "Epoch: 28/50 Discriminator Loss: 1.9691 Generator Loss:10.5769 IDS Loss: 0.3821\n",
            "Epoch: 29/50 Discriminator Loss: 1.7418 Generator Loss: 8.3913 IDS Loss: 0.3976\n",
            "Epoch: 30/50 Discriminator Loss: 1.9796 Generator Loss: 6.8932 IDS Loss: 0.3824\n",
            "Epoch: 31/50 Discriminator Loss: 1.7934 Generator Loss: 5.1231 IDS Loss: 0.3850\n",
            "Epoch: 32/50 Discriminator Loss: 1.9293 Generator Loss: 4.4399 IDS Loss: 0.3766\n",
            "Epoch: 33/50 Discriminator Loss: 1.7447 Generator Loss: 3.2623 IDS Loss: 0.3837\n",
            "Epoch: 34/50 Discriminator Loss: 1.8246 Generator Loss: 3.0209 IDS Loss: 0.3736\n",
            "Epoch: 35/50 Discriminator Loss: 1.8296 Generator Loss: 2.9189 IDS Loss: 0.3793\n",
            "Epoch: 36/50 Discriminator Loss: 1.8686 Generator Loss: 3.6400 IDS Loss: 0.3726\n",
            "Epoch: 37/50 Discriminator Loss: 1.8395 Generator Loss: 3.1721 IDS Loss: 0.3691\n",
            "Epoch: 38/50 Discriminator Loss: 1.7616 Generator Loss: 3.5566 IDS Loss: 0.3764\n",
            "Epoch: 39/50 Discriminator Loss: 1.7902 Generator Loss: 3.0875 IDS Loss: 0.3716\n",
            "Epoch: 40/50 Discriminator Loss: 1.7137 Generator Loss: 3.2583 IDS Loss: 0.3393\n",
            "Epoch: 41/50 Discriminator Loss: 1.7213 Generator Loss: 3.0606 IDS Loss: 0.3388\n",
            "Epoch: 42/50 Discriminator Loss: 1.6787 Generator Loss: 3.1272 IDS Loss: 0.3422\n",
            "Epoch: 43/50 Discriminator Loss: 1.6592 Generator Loss: 3.2194 IDS Loss: 0.3363\n",
            "Epoch: 44/50 Discriminator Loss: 1.5608 Generator Loss: 3.3227 IDS Loss: 0.3385\n",
            "Epoch: 45/50 Discriminator Loss: 1.5962 Generator Loss: 3.4005 IDS Loss: 0.3368\n",
            "Epoch: 46/50 Discriminator Loss: 1.6282 Generator Loss: 3.2788 IDS Loss: 0.3333\n",
            "Epoch: 47/50 Discriminator Loss: 1.6037 Generator Loss: 3.2027 IDS Loss: 0.3347\n",
            "Epoch: 48/50 Discriminator Loss: 1.5769 Generator Loss: 3.1438 IDS Loss: 0.3317\n",
            "Epoch: 49/50 Discriminator Loss: 1.7422 Generator Loss: 3.5008 IDS Loss: 0.3318\n",
            "Epoch: 50/50 Discriminator Loss: 1.5196 Generator Loss: 3.2464 IDS Loss: 0.3332\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}