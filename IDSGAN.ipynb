{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDSGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1dgshOPJWRGAg2Dhr99T9BbzpWxWKU934",
      "authorship_tag": "ABX9TyMF2ybDMSH2aWLmWXx4pjMd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Poojithavb/CMPE258-Project/blob/master/IDSGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqFrcKU9l2n-",
        "colab_type": "code",
        "outputId": "3cd47a1e-294d-4d06-b380-c7c037ac359b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVSStAKhfVzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") \n",
        "import sys\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.constraints import Constraint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT_GHH3EgKV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pwd = os.getcwd()\n",
        "path = pwd + '/drive/My Drive/MachineLearningCVE'\n",
        "dfs = []\n",
        "filenames = glob(path + \"/*.csv\")\n",
        "filenames.sort()\n",
        "for filename in filenames:\n",
        "    dfs.append(pd.read_csv(filename))\n",
        "    \n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "df['Flow Bytes/s'] = df['Flow Bytes/s'].astype(float)\n",
        "df[' Flow Packets/s'] = df[' Flow Packets/s'].astype(float)\n",
        "df = df.fillna(0)\n",
        "df = df.replace([np.inf, -np.inf], sys.maxsize)\n",
        "#print(\"Columns which have NaN or NULLS: \")\n",
        "#for col in df.columns:\n",
        "#    print('%s: %d' % (col, df[col].isna().sum()))\n",
        "substr = 'ï¿½'\n",
        "df[' Label'] = df[' Label'].str.replace(substr, '-')\n",
        "encoding = {\n",
        "        \" Label\": {\"BENIGN\": -1, \"FTP-Patator\": 1, \"SSH-Patator\": 1, \n",
        "                   \"DoS slowloris\": 1, \"DoS Slowhttptest\": 1, \"DoS Hulk\": 1, \n",
        "                   \"DoS GoldenEye\": 1, \"Heartbleed\": 1, \n",
        "                   \"Web Attack - Brute Force\" : 1, \"Web Attack - XSS\": 1,\n",
        "                   \"Web Attack - Sql Injection\": 1, \"Infiltration\" : 1, \n",
        "                   \"Bot\":1, \"DDoS\":1,\"PortScan\": 1}\n",
        "        }\n",
        "df.replace(encoding, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7IoAtWThJDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_data = df.drop(columns=[\n",
        "    \"Fwd Avg Bytes/Bulk\",              \n",
        "    \" Fwd Avg Packets/Bulk\",           \n",
        "    \" Fwd Avg Bulk Rate\",              \n",
        "    \" Bwd Avg Bytes/Bulk\",             \n",
        "    \" Bwd Avg Packets/Bulk\", \n",
        "    \"Bwd Avg Bulk Rate\", \n",
        "    \" Bwd PSH Flags\", \n",
        "    \" Bwd URG Flags\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maxQLtenVdyJ",
        "colab_type": "code",
        "outputId": "326535c6-cf56-4f2c-b47a-9697e00cbcd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "full_data.columns"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([' Destination Port', ' Flow Duration', ' Total Fwd Packets',\n",
              "       ' Total Backward Packets', 'Total Length of Fwd Packets',\n",
              "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max',\n",
              "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean',\n",
              "       ' Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
              "       ' Bwd Packet Length Min', ' Bwd Packet Length Mean',\n",
              "       ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s',\n",
              "       ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min',\n",
              "       'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max',\n",
              "       ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std',\n",
              "       ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Fwd URG Flags',\n",
              "       ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s',\n",
              "       ' Bwd Packets/s', ' Min Packet Length', ' Max Packet Length',\n",
              "       ' Packet Length Mean', ' Packet Length Std', ' Packet Length Variance',\n",
              "       'FIN Flag Count', ' SYN Flag Count', ' RST Flag Count',\n",
              "       ' PSH Flag Count', ' ACK Flag Count', ' URG Flag Count',\n",
              "       ' CWE Flag Count', ' ECE Flag Count', ' Down/Up Ratio',\n",
              "       ' Average Packet Size', ' Avg Fwd Segment Size',\n",
              "       ' Avg Bwd Segment Size', ' Fwd Header Length.1', 'Subflow Fwd Packets',\n",
              "       ' Subflow Fwd Bytes', ' Subflow Bwd Packets', ' Subflow Bwd Bytes',\n",
              "       'Init_Win_bytes_forward', ' Init_Win_bytes_backward',\n",
              "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean',\n",
              "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
              "       ' Idle Max', ' Idle Min', ' Label'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSU4fQ0thM5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "functional_cols = ['Idle Mean',\n",
        " ' Idle Std',\n",
        " ' Idle Min',\n",
        " ' Idle Max',\n",
        " ' Avg Bwd Segment Size',\n",
        " ' Average Packet Size',\n",
        " ' URG Flag Count',\n",
        " 'FIN Flag Count',\n",
        " ' Packet Length Variance',\n",
        " ' Packet Length Std',\n",
        " ' Packet Length Mean',\n",
        " ' Max Packet Length',\n",
        " ' Min Packet Length',\n",
        " ' Fwd IAT Max',\n",
        " ' Fwd IAT Std',\n",
        " ' Fwd IAT Mean',\n",
        " 'Fwd IAT Total',\n",
        " ' Flow IAT Max',\n",
        " ' Flow IAT Mean',\n",
        " ' Flow IAT Std',\n",
        " ' Bwd Packet Length Std',\n",
        " 'Bwd Packet Length Max',\n",
        " ' Bwd Packet Length Min',\n",
        " ' Bwd Packet Length Mean',\n",
        " ' Flow Duration']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXLpyI41hSQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "functional_features = np.array([col in functional_cols for col in full_data.columns[:-1]], dtype=np.float64)\n",
        "nonFunctional_features = np.array([col not in functional_cols for col in full_data.columns[:-1]], dtype=np.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyRMfIcvpPNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def checker(arr):\n",
        "  count = 0\n",
        "  for i in arr:\n",
        "    if i != 1:\n",
        "      count += 1\n",
        "  return count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1ceZTpUhj0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _toarray(X, Y):\n",
        "    X = X.astype(float)\n",
        "    X = np.asarray(X)\n",
        "    Y = np.asarray(Y)\n",
        "    Y = Y.reshape(-1, 1)\n",
        "    #Y = to_categorical(Y, 2)\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X)\n",
        "    X = scaler.transform(X)\n",
        "    return X, Y\n",
        "    \n",
        "def _getData(df):\n",
        "    _all = df\n",
        "    _norm = df.loc[df[' Label'] != 1]\n",
        "    _mal = df.loc[df[' Label'] == 1]\n",
        "    #print(checker(_mal[' Label']))\n",
        "    l_all = _all[' Label'][:,None]\n",
        "    l_norm = _norm[' Label']\n",
        "    l_mal = _mal[' Label']\n",
        "    _all = _all.drop([' Label'], axis=1)\n",
        "    _mal = _mal.drop([' Label'], axis=1)\n",
        "    _norm = _norm.drop([' Label'], axis=1)\n",
        "    data_all, label_all = _toarray(_all, l_all)\n",
        "    data_mal, label_mal = _toarray(_mal, l_mal)\n",
        "    #print(checker(label_mal))\n",
        "    data_norm, label_norm = _toarray(_norm, l_norm)\n",
        "\n",
        "    return data_all, label_all, data_mal, label_mal, data_norm, label_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWX7yc2Eho_2",
        "colab_type": "code",
        "outputId": "c858ebef-4de9-48e3-b25f-0b7e1585ab62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_res, y_res = rus.fit_resample(full_data.iloc[:,:-1], full_data.iloc[:,-1])\n",
        "print('Resampled dataset shape %s' % Counter(y_res))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resampled dataset shape Counter({-1: 557646, 1: 557646})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EpM4iFNiDVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "dataframe = pd.DataFrame(X_res)\n",
        "dataframe[' Label'] = y_res \n",
        "dataframe = shuffle(dataframe).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNy_rE99iLtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_all, Y_all, malicious_data, Y_mal, normal_data, Y_norm = _getData(dataframe)\n",
        "n_data = normal_data[int(len(normal_data)/2):]\n",
        "m_data = np.vstack((malicious_data, malicious_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWT_7kfR9Nrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mal_norm = np.vstack((m_data, n_data)) \n",
        "labels = np.hstack((np.ones(len(m_data)), -1 * np.ones(len(n_data))))\n",
        "labels = labels.reshape(-1, 1)\n",
        "data = np.hstack((mal_norm, labels))\n",
        "np.random.shuffle(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nai1YhciOOo",
        "colab_type": "code",
        "outputId": "3c1bd248-be9c-430f-99f8-a94787636a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Malicious_data\",malicious_data.shape)\n",
        "print(\"Normal_data\",normal_data.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Malicious_data (557646, 70)\n",
            "Normal_data (557646, 70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeYXnc1n9OhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _getIDSdata(arr):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for i in range(0, len(arr)):\n",
        "        a = arr[i][:-1]\n",
        "        data.append(a)\n",
        "        \n",
        "        b = arr[i][-1:]\n",
        "        labels.append(b)\n",
        "        \n",
        "    X = np.asarray(data)\n",
        "    y = np.asarray(labels)\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QttOrbv59UmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = _getIDSdata(data)\n",
        "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7ON_F-yiXwW",
        "colab_type": "code",
        "outputId": "cdb83189-2b43-4371-e026-f1fa08c9fa59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
              "                     criterion='gini', max_depth=None, max_features='auto',\n",
              "                     max_leaf_nodes=None, max_samples=None,\n",
              "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                     min_samples_leaf=1, min_samples_split=2,\n",
              "                     min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                     n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
              "                     warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XZnmIeHiaDK",
        "colab_type": "code",
        "outputId": "7ba0b3cb-e013-40ff-9d41-855f76bf91ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mal_pred = clf.predict(malicious_data)\n",
        "print(\"F1 Score Malcious Data: \", f1_score(np.ones_like(mal_pred), mal_pred, average='micro'))\n",
        "print(\"MSE Malcious Data: \", mean_squared_error(np.ones_like(mal_pred), mal_pred))\n",
        "norm_pred = clf.predict(normal_data)\n",
        "print(\"F1 Score Normal Data: \", f1_score(np.zeros_like(norm_pred), norm_pred, average='micro'))\n",
        "print(\"MSE Normal Data: \", mean_squared_error(np.zeros_like(norm_pred), norm_pred))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 Score Malcious Data:  1.0\n",
            "MSE Malcious Data:  0.0\n",
            "F1 Score Normal Data:  0.0\n",
            "MSE Normal Data:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL_lpD0hlyH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ClipConstraint(Constraint):\n",
        "\t# set clip value when initialized\n",
        "\tdef __init__(self, clip_value):\n",
        "\t\tself.clip_value = clip_value\n",
        " \n",
        "\t# clip model weights to hypercube\n",
        "\tdef __call__(self, weights):\n",
        "\t\treturn backend.clip(weights, -self.clip_value, self.clip_value)\n",
        " \n",
        "\t# get the config\n",
        "\tdef get_config(self):\n",
        "\t\treturn {'clip_value': self.clip_value}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feCUmxCgiqjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _generator(input=(70,)):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(32, input_shape=(70,)))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    #model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(64))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    #model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(128))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    #model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(256))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    #model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(np.prod(70,)))\n",
        "    #model.add(layers.Activation('sigmoid'))\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HybE1Jfiit6L",
        "colab_type": "code",
        "outputId": "9278a1ca-6e63-46ec-ebbc-e9ac11124b59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "g = _generator()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32)                2272      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 70)                17990     \n",
            "=================================================================\n",
            "Total params: 65,638\n",
            "Trainable params: 64,678\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCpW0IPCiweu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _discriminator():\n",
        "    #const = ClipConstraint(0.01)\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(128, kernel_constraint=const, input_shape=(70, )))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(128, kernel_constraint=const))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(64, kernel_constraint=const))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(1))\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quU3VaJ5iyWN",
        "colab_type": "code",
        "outputId": "88d87f50-a270-46e2-be9b-5560ebb36164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "d = _discriminator()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 128)               9088      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 35,201\n",
            "Trainable params: 34,561\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvKhI5v7i1dU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(data, batch_size):\n",
        "    indices = np.random.randint(low=0, high=len(data), size=batch_size)\n",
        "    np.random.shuffle(indices)\n",
        "    batch_data = data[indices]\n",
        "    return batch_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD2Fn_Tvi3xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_latent_points(latent_dim, n):\n",
        "    x_input = np.random.uniform(0, 1, (latent_dim * n))\n",
        "    x_input = x_input.reshape(n, latent_dim)\n",
        "    return x_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EQRbJZsLCj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _getPredictedNormalAdversial(arr):\n",
        "  for i in range(0, len(arr)):\n",
        "    n = []\n",
        "    a = []\n",
        "    if arr[i][-1:] == 0:\n",
        "      n.append(arr[i][:-1])\n",
        "    if arr[i][-1:] == 1:\n",
        "      a.append(arr[i][:-1])\n",
        "\n",
        "    normal = np.asarray(n)\n",
        "    attack = np.asarray(a)\n",
        "\n",
        "    return normal, attack"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc-Dj45MjCR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ids_compute(ids_classifer, ids_samples, batch_size):\n",
        "    a = tf.make_tensor_proto(ids_samples)\n",
        "    ids_samples = tf.make_ndarray(a)\n",
        "    #print(ids_samples.shape)\n",
        "    ids_preds = ids_classifer.predict(ids_samples)  \n",
        "    ids_preds = ids_preds.reshape(-1, 1)\n",
        "    mse = mean_squared_error(np.hstack([np.ones(int(len(ids_preds)/2)), -1 * np.ones(int(len(ids_preds)/2))]), ids_preds)\n",
        "    ids_stack = np.hstack((ids_samples, ids_preds))\n",
        "    #print(ids_stack.shape)\n",
        "    ids_normal, ids_attack = _getPredictedNormalAdversial(ids_stack)\n",
        "    #print(\"Predicted Normals: \", ids_normal.shape[0])\n",
        "    #print(\"Predicted Attacks:\", ids_attack.shape[0])\n",
        "    predicted_attacks = ids_preds[:int(len(ids_preds)/2)]\n",
        "    predicted_normals = ids_preds[int(len(ids_preds)/2):]   \n",
        "    #print(\"Predicted Attacks:\", predicted_attacks)\n",
        "    #print(\"Predicted Normal:\", predicted_normals)\n",
        "    ids_normal = tf.convert_to_tensor(ids_normal, float)\n",
        "    ids_attack = tf.convert_to_tensor(ids_attack, float)\n",
        "    ids_samples = tf.convert_to_tensor(ids_samples, float)\n",
        "    predicted_attacks = tf.convert_to_tensor(predicted_attacks, float)\n",
        "    predicted_normals = tf.convert_to_tensor(predicted_normals, float)\n",
        "    \n",
        "\n",
        "    return ids_samples, ids_normal, ids_attack, predicted_normals, predicted_attacks, mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W_rnoltqPEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _setTheshold(arr):\n",
        "  a = tf.where(tf.less(arr, 0.0), 0, arr) \n",
        "  arr = tf.where(tf.greater(a, 1.0), 1, a)\n",
        "  \n",
        "  return arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9__Dvosi88a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "def generator_loss(d_generated):\n",
        "    return tf.reduce_mean(d_generated) * 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7xSzJvRi6Z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(d_normal, d_generated, predicted_attacks, predicted_normals):\n",
        "    return tf.math.subtract((tf.reduce_mean(predicted_normals * d_normal)), \n",
        "                            tf.reduce_mean(predicted_attacks * d_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-PC0OD8iQYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_loss(losses):\n",
        "    labels = ['Discriminator', 'Generator', 'IDS-Classifier']\n",
        "    losses = np.array(losses)    \n",
        "    \n",
        "    fig, ax = plt.subplots()\n",
        "    plt.plot(losses.T[0], label='Discriminator')\n",
        "    plt.plot(losses.T[1], label='Generator')\n",
        "    plt.plot(losses.T[2], label='IDS-Classifier')\n",
        "    plt.title(\"Training Losses\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ici-CryFjI5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs=10, batch_size=1000, functional_features=functional_features,\n",
        "          nonFunctional_features=nonFunctional_features):\n",
        "    \n",
        "    losses = []\n",
        "    ids, generator, discriminator = clf, g, d\n",
        "    noise = generate_latent_points(70, batch_size)\n",
        "\n",
        "    generator_optimizer = tf.keras.optimizers.RMSprop(1e-4) #change to \n",
        "    discriminator_optimizer = tf.keras.optimizers.RMSprop(1e-4) #change to \n",
        "    \n",
        "    for e in range(epochs):\n",
        "        for i in range(int(normal_data.shape[0]/batch_size)):\n",
        "            normal_samples = get_batch(normal_data, batch_size=batch_size)\n",
        "            malicious_samples = get_batch(malicious_data, batch_size=batch_size)\n",
        "            generator_in = malicious_samples + noise\n",
        "            #generator_in = noise\n",
        "                        \n",
        "            normal_samples = tf.convert_to_tensor(normal_samples, float)\n",
        "            malicious_samples = tf.convert_to_tensor(malicious_samples, float)\n",
        "            functional_features = tf.convert_to_tensor(functional_features, float)\n",
        "            nonFunctional_features = tf.convert_to_tensor(nonFunctional_features, float)\n",
        "            generator_in = tf.convert_to_tensor(generator_in, float)\n",
        "\n",
        "\n",
        "            \n",
        "            with tf.GradientTape() as gTape, tf.GradientTape() as dTape:\n",
        "                generated_maliciousSamples = tf.math.add(tf.math.multiply(malicious_samples, \n",
        "                                                                          functional_features), \n",
        "                                                         _setTheshold(tf.math.multiply(generator(generator_in), \n",
        "                                                                     nonFunctional_features)))\n",
        "                \n",
        "                #print(generated_maliciousSamples)\n",
        "            \n",
        "                ids_samples = tf.concat([generated_maliciousSamples, normal_samples], 0)\n",
        "                #print(ids_samples.shape)\n",
        "#                 ids_samples = tf.Session().run(ids_samples)\n",
        "#                 ids_predictions = ids.predict(ids_samples)\n",
        "                _, _, _, predicted_normals, predicted_attacks, ids_loss = ids_compute(ids, ids_samples, \n",
        "                                                                                  batch_size=batch_size)\n",
        "                #print(ids_predictedAttacks.shape)\n",
        "                #print(\"ids loss:\", ids_loss)\n",
        "                     \n",
        "                d_normal = discriminator(normal_samples)\n",
        "                d_gAttack = discriminator(generated_maliciousSamples)\n",
        "                \n",
        "                d_loss = discriminator_loss(d_normal, d_gAttack, predicted_attacks, predicted_normals)\n",
        "                #print(\"dloss:\", d_loss)\n",
        "\n",
        "                #d_gAttack = discriminator(generated_maliciousSamples)\n",
        "            \n",
        "                g_loss = generator_loss(d_gAttack)\n",
        "                #print(\"gloss:\", g_loss)\n",
        "    \n",
        "            gGradients = gTape.gradient(g_loss, generator.trainable_variables)\n",
        "            dGradients = dTape.gradient(d_loss, discriminator.trainable_variables)\n",
        "\n",
        "            generator_optimizer.apply_gradients(zip(gGradients, generator.trainable_variables))\n",
        "            discriminator_optimizer.apply_gradients(zip(dGradients, discriminator.trainable_variables))\n",
        "            losses.append((d_loss, g_loss, ids_loss))\n",
        "\n",
        "        print(\"Epoch:{:>3}/{} Discriminator Loss:{:>7.4f} Generator Loss:{:>7.4f} IDS Loss:{:>7.4f}\".format(\n",
        "            e+1, epochs, d_loss, g_loss, ids_loss)) \n",
        "    \n",
        "    plot_loss(losses)\n",
        "\n",
        "    return generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TamaBnd3nCxO",
        "colab_type": "code",
        "outputId": "8af087f3-37e1-4e68-911e-a60e93707223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1/10 Discriminator Loss:-22472.1250 Generator Loss:9150.7998 IDS Loss: 0.0000\n",
            "Epoch:  2/10 Discriminator Loss:-62261.2422 Generator Loss:25296.9941 IDS Loss: 0.0000\n",
            "Epoch:  3/10 Discriminator Loss:-145483.8438 Generator Loss:58882.7109 IDS Loss: 0.0000\n",
            "Epoch:  4/10 Discriminator Loss:-304641.5000 Generator Loss:122249.2344 IDS Loss: 0.0000\n",
            "Epoch:  5/10 Discriminator Loss:-581211.3125 Generator Loss:237153.7031 IDS Loss: 0.0000\n",
            "Epoch:  6/10 Discriminator Loss:-1039079.1250 Generator Loss:422041.4375 IDS Loss: 0.0000\n",
            "Epoch:  7/10 Discriminator Loss:-1745338.5000 Generator Loss:699621.4375 IDS Loss: 0.0020\n",
            "Epoch:  8/10 Discriminator Loss:-2831275.2500 Generator Loss:1156243.7500 IDS Loss: 0.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ8jx3t57zAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}