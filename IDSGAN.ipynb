{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDSGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1dgshOPJWRGAg2Dhr99T9BbzpWxWKU934",
      "authorship_tag": "ABX9TyPiXxmtNAkwNSXwtKor9/zi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Poojithavb/CMPE258-Project/blob/master/IDSGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqFrcKU9l2n-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94cd5d7a-a6ff-4613-d964-c388d660304f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVSStAKhfVzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") \n",
        "import sys\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT_GHH3EgKV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pwd = os.getcwd()\n",
        "path = pwd + '/drive/My Drive/MachineLearningCVE'\n",
        "dfs = []\n",
        "filenames = glob(path + \"/*.csv\")\n",
        "filenames.sort()\n",
        "for filename in filenames:\n",
        "    dfs.append(pd.read_csv(filename))\n",
        "    \n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "df['Flow Bytes/s'] = df['Flow Bytes/s'].astype(float)\n",
        "df[' Flow Packets/s'] = df[' Flow Packets/s'].astype(float)\n",
        "df = df.fillna(0)\n",
        "df = df.replace([np.inf, -np.inf], sys.maxsize)\n",
        "#print(\"Columns which have NaN or NULLS: \")\n",
        "#for col in df.columns:\n",
        "#    print('%s: %d' % (col, df[col].isna().sum()))\n",
        "substr = 'ï¿½'\n",
        "df[' Label'] = df[' Label'].str.replace(substr, '-')\n",
        "encoding = {\n",
        "        \" Label\": {\"BENIGN\": 0, \"FTP-Patator\": 1, \"SSH-Patator\": 1, \n",
        "                   \"DoS slowloris\": 1, \"DoS Slowhttptest\": 1, \"DoS Hulk\": 1, \n",
        "                   \"DoS GoldenEye\": 1, \"Heartbleed\": 1, \n",
        "                   \"Web Attack - Brute Force\" : 1, \"Web Attack - XSS\": 1,\n",
        "                   \"Web Attack - Sql Injection\": 1, \"Infiltration\" : 1, \n",
        "                   \"Bot\":1, \"DDoS\":1,\"PortScan\": 1}\n",
        "        }\n",
        "df.replace(encoding, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7IoAtWThJDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_data = df.drop(columns=[\n",
        "    \"Fwd Avg Bytes/Bulk\",              \n",
        "    \" Fwd Avg Packets/Bulk\",           \n",
        "    \" Fwd Avg Bulk Rate\",              \n",
        "    \" Bwd Avg Bytes/Bulk\",             \n",
        "    \" Bwd Avg Packets/Bulk\", \n",
        "    \"Bwd Avg Bulk Rate\", \n",
        "    \" Bwd PSH Flags\", \n",
        "    \" Bwd URG Flags\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maxQLtenVdyJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "905bb4af-1c2c-4fa9-ff55-01951cd8e1ca"
      },
      "source": [
        "full_data.columns"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([' Destination Port', ' Flow Duration', ' Total Fwd Packets',\n",
              "       ' Total Backward Packets', 'Total Length of Fwd Packets',\n",
              "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max',\n",
              "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean',\n",
              "       ' Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
              "       ' Bwd Packet Length Min', ' Bwd Packet Length Mean',\n",
              "       ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s',\n",
              "       ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min',\n",
              "       'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max',\n",
              "       ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std',\n",
              "       ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Fwd URG Flags',\n",
              "       ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s',\n",
              "       ' Bwd Packets/s', ' Min Packet Length', ' Max Packet Length',\n",
              "       ' Packet Length Mean', ' Packet Length Std', ' Packet Length Variance',\n",
              "       'FIN Flag Count', ' SYN Flag Count', ' RST Flag Count',\n",
              "       ' PSH Flag Count', ' ACK Flag Count', ' URG Flag Count',\n",
              "       ' CWE Flag Count', ' ECE Flag Count', ' Down/Up Ratio',\n",
              "       ' Average Packet Size', ' Avg Fwd Segment Size',\n",
              "       ' Avg Bwd Segment Size', ' Fwd Header Length.1', 'Subflow Fwd Packets',\n",
              "       ' Subflow Fwd Bytes', ' Subflow Bwd Packets', ' Subflow Bwd Bytes',\n",
              "       'Init_Win_bytes_forward', ' Init_Win_bytes_backward',\n",
              "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean',\n",
              "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
              "       ' Idle Max', ' Idle Min', ' Label'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSU4fQ0thM5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "functional_cols = ['Idle Mean',\n",
        " ' Idle Std',\n",
        " ' Idle Min',\n",
        " ' Idle Max',\n",
        " ' Avg Bwd Segment Size',\n",
        " ' Average Packet Size',\n",
        " ' URG Flag Count',\n",
        " 'FIN Flag Count',\n",
        " ' Packet Length Variance',\n",
        " ' Packet Length Std',\n",
        " ' Packet Length Mean',\n",
        " ' Max Packet Length',\n",
        " ' Min Packet Length',\n",
        " ' Fwd IAT Max',\n",
        " ' Fwd IAT Std',\n",
        " ' Fwd IAT Mean',\n",
        " 'Fwd IAT Total',\n",
        " ' Flow IAT Max',\n",
        " ' Flow IAT Mean',\n",
        " ' Flow IAT Std',\n",
        " ' Bwd Packet Length Std',\n",
        " 'Bwd Packet Length Max',\n",
        " ' Bwd Packet Length Min',\n",
        " ' Bwd Packet Length Mean',\n",
        " ' Flow Duration']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXLpyI41hSQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "functional_features = np.array([col in functional_cols for col in full_data.columns[:-1]], dtype=np.float64)\n",
        "nonFunctional_features = np.array([col not in functional_cols for col in full_data.columns[:-1]], dtype=np.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1ceZTpUhj0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _toarray(X, Y):\n",
        "    X = X.astype(float)\n",
        "    X = np.asarray(X)\n",
        "    Y = np.asarray(Y)\n",
        "    #Y = to_categorical(Y, 2)\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X)\n",
        "    X = scaler.transform(X)\n",
        "    return X, Y\n",
        "    \n",
        "def _getData(df):\n",
        "    _all = df\n",
        "    _norm = df.loc[df[' Label'] == 0]\n",
        "    _mal = df.loc[df[' Label'] != 0]\n",
        "    l_all = _all[' Label'][:,None]\n",
        "    l_norm = _norm[' Label']\n",
        "    l_mal = _mal[' Label']\n",
        "    _all = _all.drop([' Label'], axis=1)\n",
        "    _mal = _mal.drop([' Label'], axis=1)\n",
        "    _norm = _norm.drop([' Label'], axis=1)\n",
        "    data_all, label_all = _toarray(_all, l_all)\n",
        "    data_mal, label_mal = _toarray(_mal, l_mal)\n",
        "    data_norm, label_norm = _toarray(_norm, l_norm)\n",
        "    return data_all, label_all, data_mal, label_mal, data_norm, label_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWX7yc2Eho_2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "999bfb6b-03b4-4803-ab0b-ef45a1f15065"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_res, y_res = rus.fit_resample(full_data.iloc[:,:-1], full_data.iloc[:,-1])\n",
        "print('Resampled dataset shape %s' % Counter(y_res))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resampled dataset shape Counter({0: 557646, 1: 557646})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EpM4iFNiDVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataframe = pd.DataFrame(X_res)\n",
        "dataframe[' Label'] = y_res "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNy_rE99iLtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_all, Y_all, malicious_data, Y_mal, normal_data, Y_norm = _getData(dataframe)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nai1YhciOOo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3e02d49a-f0fb-4a68-d036-312d3c6366ca"
      },
      "source": [
        "print(\"Malicious_data\",malicious_data.shape)\n",
        "print(\"Normal_data\",normal_data.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Malicious_data (557646, 70)\n",
            "Normal_data (557646, 70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9myWtqEqiQjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train,y_test = train_test_split(X_all,Y_all,test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7ON_F-yiXwW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "340e6c64-1f5a-430f-8d1e-66e6049b2e10"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
              "                     criterion='gini', max_depth=None, max_features='auto',\n",
              "                     max_leaf_nodes=None, max_samples=None,\n",
              "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                     min_samples_leaf=1, min_samples_split=2,\n",
              "                     min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                     n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
              "                     warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XZnmIeHiaDK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cef4af8a-4da6-4545-dd1b-73d377d1610a"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"F1 Score: \", f1_score(y_test, y_pred, average='micro'))\n",
        "print(\"MSE: \", mean_squared_error(y_test, y_pred))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 Score:  0.9986640365820856\n",
            "MSE:  0.0013359634179144625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feCUmxCgiqjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _generator(input=(70,)):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(32, input_shape=(70,)))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    #model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(64))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    #model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(128))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    #model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(256))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    #model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(np.prod(70,)))\n",
        "    #model.add(layers.Activation('sigmoid'))\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HybE1Jfiit6L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "5c66bb10-1605-4cf5-bb1d-fc6276a6ff76"
      },
      "source": [
        "g = _generator()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 32)                2272      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 70)                17990     \n",
            "=================================================================\n",
            "Total params: 65,638\n",
            "Trainable params: 64,678\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCpW0IPCiweu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _discriminator():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(128, input_shape=(70, )))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(128))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(64))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(1))\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quU3VaJ5iyWN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "2b3a9b55-0d17-4f9e-d656-8f07b4eec5f8"
      },
      "source": [
        "d = _discriminator()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_14 (Dense)             (None, 128)               9088      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 35,201\n",
            "Trainable params: 34,561\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvKhI5v7i1dU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(data, batch_size):\n",
        "    indices = np.random.randint(low=0, high=len(data), size=batch_size)\n",
        "    np.random.shuffle(indices)\n",
        "    batch_data = data[indices]\n",
        "    return batch_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD2Fn_Tvi3xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_latent_points(latent_dim, n):\n",
        "    x_input = np.random.uniform(0, 1, (latent_dim * n))\n",
        "    x_input = x_input.reshape(n, latent_dim)\n",
        "    return x_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9__Dvosi88a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "def generator_loss(outputs):\n",
        "    return cross_entropy(tf.ones_like(outputs), outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7xSzJvRi6Z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(ids_predicts, d_predicts):    \n",
        "    return cross_entropy(ids_predicts, d_predicts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc-Dj45MjCR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ids_compute(ids_classifer, ids_samples, batch_size):\n",
        "    a = tf.make_tensor_proto(ids_samples)\n",
        "    ids_samples = tf.make_ndarray(a)\n",
        "    ids_preds = ids_classifer.predict(ids_samples)\n",
        "    mse = mean_squared_error(np.hstack([np.ones(int(len(ids_preds)/2)), np.zeros(int(len(ids_preds)/2))]), ids_preds)\n",
        "    ids_predictions = tf.convert_to_tensor(ids_preds, float)\n",
        "    ids_samples = tf.convert_to_tensor(ids_samples, float)\n",
        "\n",
        "    return ids_samples, ids_predictions, mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W_rnoltqPEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _setTheshold(arr):\n",
        "  a = tf.where(tf.less(arr, 0.0), 0, arr) \n",
        "  arr = tf.where(tf.greater(a, 1.0), 1, a)\n",
        "  \n",
        "  return arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ici-CryFjI5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs=50, batch_size=10000, functional_features=functional_features,\n",
        "          nonFunctional_features=nonFunctional_features):\n",
        "    \n",
        "    losses = []\n",
        "    ids, generator, discriminator = clf, g, d\n",
        "    noise = generate_latent_points(70, batch_size)\n",
        "\n",
        "    generator_optimizer = tf.keras.optimizers.RMSprop(1e-4)\n",
        "    discriminator_optimizer = tf.keras.optimizers.RMSprop(1e-3)\n",
        "    \n",
        "    for e in range(epochs):\n",
        "        for i in range(int(normal_data.shape[0]/batch_size)):\n",
        "            normal_samples = get_batch(normal_data, batch_size=batch_size)\n",
        "            malicious_samples = get_batch(malicious_data, batch_size=batch_size)\n",
        "            generator_in = malicious_samples + noise\n",
        "                        \n",
        "            normal_samples = tf.convert_to_tensor(normal_samples, float)\n",
        "            malicious_samples = tf.convert_to_tensor(malicious_samples, float)\n",
        "            functional_features = tf.convert_to_tensor(functional_features, float)\n",
        "            nonFunctional_features = tf.convert_to_tensor(nonFunctional_features, float)\n",
        "            generator_in = tf.convert_to_tensor(generator_in, float)\n",
        "\n",
        "\n",
        "            \n",
        "            with tf.GradientTape() as gTape, tf.GradientTape() as dTape:\n",
        "                generated_maliciousSamples = tf.math.add(tf.math.multiply(malicious_samples, \n",
        "                                                                          functional_features), \n",
        "                                                         _setTheshold(tf.math.multiply(generator(generator_in), \n",
        "                                                                     nonFunctional_features)))\n",
        "                \n",
        "                #print(generated_maliciousSamples)\n",
        "            \n",
        "                ids_samples = tf.concat([generated_maliciousSamples, normal_samples], 0)\n",
        "#                 ids_samples = tf.Session().run(ids_samples)\n",
        "#                 ids_predictions = ids.predict(ids_samples)\n",
        "                ids_samples, ids_predictions, ids_loss = ids_compute(ids, ids_samples, batch_size=batch_size)\n",
        "                #print(\"ids loss:\", ids_loss)\n",
        "                     \n",
        "                d_predictions = discriminator(ids_samples)\n",
        "            \n",
        "                d_loss = discriminator_loss(ids_predictions, d_predictions)*10\n",
        "                #print(\"dloss:\", d_loss)\n",
        "\n",
        "\n",
        "                disGenOut = discriminator(generated_maliciousSamples)\n",
        "            \n",
        "                g_loss = generator_loss(disGenOut)\n",
        "                #print(\"gloss:\", g_loss)\n",
        "    \n",
        "            gGradients = gTape.gradient(g_loss, generator.trainable_variables)\n",
        "            dGradients = dTape.gradient(d_loss, discriminator.trainable_variables)\n",
        "\n",
        "            generator_optimizer.apply_gradients(zip(gGradients, generator.trainable_variables))\n",
        "            discriminator_optimizer.apply_gradients(zip(dGradients, discriminator.trainable_variables))\n",
        "            losses.append((d_loss, g_loss))\n",
        "\n",
        "        print(\"Epoch:{:>3}/{} Discriminator Loss:{:>7.4f} Generator Loss:{:>7.4f} IDS Loss:{:>7.4f}\".format(\n",
        "            e+1, epochs, d_loss, g_loss, ids_loss))     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TamaBnd3nCxO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "3cff4c45-317c-462c-b27d-151fe94b71ae"
      },
      "source": [
        "train()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1/50 Discriminator Loss: 0.1446 Generator Loss: 0.8705 IDS Loss: 0.1983\n",
            "Epoch:  2/50 Discriminator Loss: 0.1808 Generator Loss: 0.2145 IDS Loss: 0.1210\n",
            "Epoch:  3/50 Discriminator Loss: 0.1864 Generator Loss: 0.8867 IDS Loss: 0.1163\n",
            "Epoch:  4/50 Discriminator Loss: 0.1511 Generator Loss: 0.3453 IDS Loss: 0.1173\n",
            "Epoch:  5/50 Discriminator Loss: 0.1435 Generator Loss: 0.4336 IDS Loss: 0.1207\n",
            "Epoch:  6/50 Discriminator Loss: 0.1295 Generator Loss: 0.5278 IDS Loss: 0.1154\n",
            "Epoch:  7/50 Discriminator Loss: 0.3875 Generator Loss: 0.2684 IDS Loss: 0.2554\n",
            "Epoch:  8/50 Discriminator Loss: 0.1699 Generator Loss: 1.0329 IDS Loss: 0.1193\n",
            "Epoch:  9/50 Discriminator Loss: 0.1341 Generator Loss: 0.6445 IDS Loss: 0.1182\n",
            "Epoch: 10/50 Discriminator Loss: 0.1311 Generator Loss: 0.6122 IDS Loss: 0.1192\n",
            "Epoch: 11/50 Discriminator Loss: 0.2194 Generator Loss: 0.1671 IDS Loss: 0.1211\n",
            "Epoch: 12/50 Discriminator Loss: 0.1304 Generator Loss: 0.6501 IDS Loss: 0.1188\n",
            "Epoch: 13/50 Discriminator Loss: 0.2501 Generator Loss: 0.1204 IDS Loss: 0.1231\n",
            "Epoch: 14/50 Discriminator Loss: 0.1600 Generator Loss: 0.3615 IDS Loss: 0.1196\n",
            "Epoch: 15/50 Discriminator Loss: 0.2575 Generator Loss: 0.1284 IDS Loss: 0.1190\n",
            "Epoch: 16/50 Discriminator Loss: 0.1150 Generator Loss: 0.9430 IDS Loss: 0.1202\n",
            "Epoch: 17/50 Discriminator Loss: 0.1523 Generator Loss: 1.3666 IDS Loss: 0.1216\n",
            "Epoch: 18/50 Discriminator Loss: 0.1126 Generator Loss: 0.8691 IDS Loss: 0.1190\n",
            "Epoch: 19/50 Discriminator Loss: 0.1254 Generator Loss: 0.9272 IDS Loss: 0.1363\n",
            "Epoch: 20/50 Discriminator Loss: 0.1130 Generator Loss: 0.7160 IDS Loss: 0.1175\n",
            "Epoch: 21/50 Discriminator Loss: 0.2070 Generator Loss: 0.2889 IDS Loss: 0.1198\n",
            "Epoch: 22/50 Discriminator Loss: 0.1223 Generator Loss: 0.7525 IDS Loss: 0.1176\n",
            "Epoch: 23/50 Discriminator Loss: 0.1109 Generator Loss: 1.1203 IDS Loss: 0.1153\n",
            "Epoch: 24/50 Discriminator Loss: 0.0993 Generator Loss: 1.0777 IDS Loss: 0.1186\n",
            "Epoch: 25/50 Discriminator Loss: 0.0974 Generator Loss: 1.0748 IDS Loss: 0.1206\n",
            "Epoch: 26/50 Discriminator Loss: 0.1202 Generator Loss: 0.7424 IDS Loss: 0.1160\n",
            "Epoch: 27/50 Discriminator Loss: 0.1060 Generator Loss: 0.9203 IDS Loss: 0.1234\n",
            "Epoch: 28/50 Discriminator Loss: 0.1515 Generator Loss: 0.5311 IDS Loss: 0.1201\n",
            "Epoch: 29/50 Discriminator Loss: 0.0893 Generator Loss: 1.0514 IDS Loss: 0.1177\n",
            "Epoch: 30/50 Discriminator Loss: 0.1110 Generator Loss: 0.9240 IDS Loss: 0.1198\n",
            "Epoch: 31/50 Discriminator Loss: 0.0856 Generator Loss: 0.9526 IDS Loss: 0.1177\n",
            "Epoch: 32/50 Discriminator Loss: 0.0875 Generator Loss: 0.9523 IDS Loss: 0.1177\n",
            "Epoch: 33/50 Discriminator Loss: 0.1183 Generator Loss: 1.6039 IDS Loss: 0.1168\n",
            "Epoch: 34/50 Discriminator Loss: 0.1028 Generator Loss: 1.0084 IDS Loss: 0.1182\n",
            "Epoch: 35/50 Discriminator Loss: 0.1256 Generator Loss: 0.7717 IDS Loss: 0.1193\n",
            "Epoch: 36/50 Discriminator Loss: 0.0895 Generator Loss: 1.5803 IDS Loss: 0.1199\n",
            "Epoch: 37/50 Discriminator Loss: 0.1486 Generator Loss: 0.6511 IDS Loss: 0.1176\n",
            "Epoch: 38/50 Discriminator Loss: 0.1408 Generator Loss: 0.6728 IDS Loss: 0.1174\n",
            "Epoch: 39/50 Discriminator Loss: 0.1068 Generator Loss: 0.9414 IDS Loss: 0.1187\n",
            "Epoch: 40/50 Discriminator Loss: 0.1286 Generator Loss: 0.8927 IDS Loss: 0.1183\n",
            "Epoch: 41/50 Discriminator Loss: 0.0917 Generator Loss: 1.1543 IDS Loss: 0.1220\n",
            "Epoch: 42/50 Discriminator Loss: 0.0744 Generator Loss: 1.7201 IDS Loss: 0.1214\n",
            "Epoch: 43/50 Discriminator Loss: 0.1224 Generator Loss: 0.9200 IDS Loss: 0.1194\n",
            "Epoch: 44/50 Discriminator Loss: 0.0666 Generator Loss: 1.9808 IDS Loss: 0.1230\n",
            "Epoch: 45/50 Discriminator Loss: 0.0561 Generator Loss: 1.5568 IDS Loss: 0.1177\n",
            "Epoch: 46/50 Discriminator Loss: 0.0684 Generator Loss: 2.0916 IDS Loss: 0.1166\n",
            "Epoch: 47/50 Discriminator Loss: 0.0475 Generator Loss: 1.6296 IDS Loss: 0.1226\n",
            "Epoch: 48/50 Discriminator Loss: 0.1878 Generator Loss: 0.6916 IDS Loss: 0.1148\n",
            "Epoch: 49/50 Discriminator Loss: 0.0281 Generator Loss: 1.7068 IDS Loss: 0.1247\n",
            "Epoch: 50/50 Discriminator Loss: 0.0365 Generator Loss: 1.8263 IDS Loss: 0.1216\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}