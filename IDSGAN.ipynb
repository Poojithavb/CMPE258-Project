{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDSGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1dgshOPJWRGAg2Dhr99T9BbzpWxWKU934",
      "authorship_tag": "ABX9TyPGkmp5nKyfT/SFbkJ0nMyq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Poojithavb/CMPE258-Project/blob/master/IDSGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqFrcKU9l2n-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "91e92d43-432c-4d48-a82a-198dd2a95927"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVSStAKhfVzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") \n",
        "import sys\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT_GHH3EgKV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pwd = os.getcwd()\n",
        "path = pwd + '/drive/My Drive/MachineLearningCVE'\n",
        "dfs = []\n",
        "filenames = glob(path + \"/*.csv\")\n",
        "filenames.sort()\n",
        "for filename in filenames:\n",
        "    dfs.append(pd.read_csv(filename))\n",
        "    \n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "df['Flow Bytes/s'] = df['Flow Bytes/s'].astype(float)\n",
        "df[' Flow Packets/s'] = df[' Flow Packets/s'].astype(float)\n",
        "df = df.fillna(0)\n",
        "df = df.replace([np.inf, -np.inf], sys.maxsize)\n",
        "#print(\"Columns which have NaN or NULLS: \")\n",
        "#for col in df.columns:\n",
        "#    print('%s: %d' % (col, df[col].isna().sum()))\n",
        "substr = 'ï¿½'\n",
        "df[' Label'] = df[' Label'].str.replace(substr, '-')\n",
        "encoding = {\n",
        "        \" Label\": {\"BENIGN\": 0, \"FTP-Patator\": 1, \"SSH-Patator\": 1, \n",
        "                   \"DoS slowloris\": 1, \"DoS Slowhttptest\": 1, \"DoS Hulk\": 1, \n",
        "                   \"DoS GoldenEye\": 1, \"Heartbleed\": 1, \n",
        "                   \"Web Attack - Brute Force\" : 1, \"Web Attack - XSS\": 1,\n",
        "                   \"Web Attack - Sql Injection\": 1, \"Infiltration\" : 1, \n",
        "                   \"Bot\":1, \"DDoS\":1,\"PortScan\": 1}\n",
        "        }\n",
        "df.replace(encoding, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7IoAtWThJDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_data = df.drop(columns=[\n",
        "    \"Fwd Avg Bytes/Bulk\",              \n",
        "    \" Fwd Avg Packets/Bulk\",           \n",
        "    \" Fwd Avg Bulk Rate\",              \n",
        "    \" Bwd Avg Bytes/Bulk\",             \n",
        "    \" Bwd Avg Packets/Bulk\", \n",
        "    \"Bwd Avg Bulk Rate\", \n",
        "    \" Bwd PSH Flags\", \n",
        "    \" Bwd URG Flags\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSU4fQ0thM5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "functional_features = ['Idle Mean',\n",
        " ' Idle Std',\n",
        " ' Idle Min',\n",
        " ' Idle Max',\n",
        " ' Avg Bwd Segment Size',\n",
        " ' Average Packet Size',\n",
        " ' URG Flag Count',\n",
        " 'FIN Flag Count',\n",
        " ' Packet Length Variance',\n",
        " ' Packet Length Std',\n",
        " ' Packet Length Mean',\n",
        " ' Max Packet Length',\n",
        " ' Min Packet Length',\n",
        " ' Fwd IAT Max',\n",
        " ' Fwd IAT Std',\n",
        " ' Fwd IAT Mean',\n",
        " 'Fwd IAT Total',\n",
        " ' Flow IAT Max',\n",
        " ' Flow IAT Mean',\n",
        " ' Flow IAT Std',\n",
        " ' Bwd Packet Length Std',\n",
        " 'Bwd Packet Length Max',\n",
        " ' Bwd Packet Length Min',\n",
        " ' Bwd Packet Length Mean',\n",
        " ' Flow Duration']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXLpyI41hSQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "functional_features = np.array([col in functional_features for col in full_data.columns[:-1]], dtype=np.float64)\n",
        "nonFunctional_features = np.array([col not in functional_features for col in full_data.columns[:-1]], dtype=np.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1ceZTpUhj0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _toarray(X, Y):\n",
        "    X = X.astype(float)\n",
        "    X = np.asarray(X)\n",
        "    Y = np.asarray(Y)\n",
        "    #Y = to_categorical(Y, 2)\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X)\n",
        "    X = scaler.transform(X)\n",
        "    return X, Y\n",
        "    \n",
        "def _getData(df):\n",
        "    _all = df\n",
        "    _norm = df.loc[df[' Label'] == 0]\n",
        "    _mal = df.loc[df[' Label'] != 0]\n",
        "    l_all = _all[' Label'][:,None]\n",
        "    l_norm = _norm[' Label']\n",
        "    l_mal = _mal[' Label']\n",
        "    _all = _all.drop([' Label'], axis=1)\n",
        "    _mal = _mal.drop([' Label'], axis=1)\n",
        "    _norm = _norm.drop([' Label'], axis=1)\n",
        "    data_all, label_all = _toarray(_all, l_all)\n",
        "    data_mal, label_mal = _toarray(_mal, l_mal)\n",
        "    data_norm, label_norm = _toarray(_norm, l_norm)\n",
        "    return data_all, label_all, data_mal, label_mal, data_norm, label_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWX7yc2Eho_2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "320dbdb0-30ad-444b-f8b0-813ea1859b59"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_res, y_res = rus.fit_resample(full_data.iloc[:,:-1], full_data.iloc[:,-1])\n",
        "print('Resampled dataset shape %s' % Counter(y_res))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resampled dataset shape Counter({0: 557646, 1: 557646})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EpM4iFNiDVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataframe = pd.DataFrame(X_res)\n",
        "dataframe[' Label'] = y_res "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNy_rE99iLtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_all, Y_all, malicious_data, Y_mal, normal_data, Y_norm = _getData(dataframe)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nai1YhciOOo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f8d2e908-3506-491e-d67a-3eb7699d5cb2"
      },
      "source": [
        "print(\"Malicious_data\",malicious_data.shape)\n",
        "print(\"Normal_data\",normal_data.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Malicious_data (557646, 70)\n",
            "Normal_data (557646, 70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9myWtqEqiQjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train,y_test = train_test_split(X_all,Y_all,test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7ON_F-yiXwW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "6a22da89-23e0-474c-d018-b02e1e59af3c"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
              "                     criterion='gini', max_depth=None, max_features='auto',\n",
              "                     max_leaf_nodes=None, max_samples=None,\n",
              "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                     min_samples_leaf=1, min_samples_split=2,\n",
              "                     min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                     n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
              "                     warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XZnmIeHiaDK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "25cac5f4-ed21-4d2b-e7fa-45cfdf66f9cb"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"F1 Score: \", f1_score(y_test, y_pred, average='micro'))\n",
        "print(\"MSE: \", mean_squared_error(y_test, y_pred))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 Score:  0.9986819689769568\n",
            "MSE:  0.0013180310230431274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feCUmxCgiqjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _generator(input=(70,)):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(32, input_shape=(70,)))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    #model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(64))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    #model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(128))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    #model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(256))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(np.prod(70,)))\n",
        "    #model.add(layers.Activation('sigmoid'))\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HybE1Jfiit6L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "dd8dd617-5980-4ce7-e7ab-6ab72a8d5991"
      },
      "source": [
        "g = _generator()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 32)                2272      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 70)                17990     \n",
            "=================================================================\n",
            "Total params: 65,638\n",
            "Trainable params: 64,678\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCpW0IPCiweu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _discriminator():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(128, input_shape=(70, )))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(128))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(64))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(1))\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quU3VaJ5iyWN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "c8fa30b0-0401-429e-866c-bbb7e1d950fc"
      },
      "source": [
        "d = _discriminator()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_14 (Dense)             (None, 128)               9088      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 35,201\n",
            "Trainable params: 34,561\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvKhI5v7i1dU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(data, batch_size):\n",
        "    indices = np.random.randint(low=0, high=len(data), size=batch_size)\n",
        "    np.random.shuffle(indices)\n",
        "    batch_data = data[indices]\n",
        "    return batch_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD2Fn_Tvi3xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_latent_points(latent_dim, n):\n",
        "    x_input = np.random.randn(latent_dim * n)\n",
        "    x_input = x_input.reshape(n, latent_dim)\n",
        "    return x_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9__Dvosi88a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "def generator_loss(outputs):\n",
        "    return cross_entropy(tf.zeros_like(outputs), outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7xSzJvRi6Z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(ids_predicts, d_predicts):    \n",
        "    return cross_entropy(ids_predicts, d_predicts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc-Dj45MjCR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ids_compute(ids_classifer, ids_samples, batch_size):\n",
        "    a = tf.make_tensor_proto(ids_samples)\n",
        "    ids_samples = tf.make_ndarray(a)\n",
        "    ids_samples[ids_samples > 0.5] = 1\n",
        "    ids_samples[ids_samples < 0.5] = 0\n",
        "    ids_preds = ids_classifer.predict(ids_samples)\n",
        "    mse = mean_squared_error(np.hstack([np.ones(int(len(ids_preds)/2)), np.zeros(int(len(ids_preds)/2))]), ids_preds)\n",
        "    ids_predictions = tf.convert_to_tensor(ids_preds, float)\n",
        "    ids_samples = tf.convert_to_tensor(ids_samples, float)\n",
        "\n",
        "    return ids_samples, ids_predictions, mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ici-CryFjI5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs=10, batch_size=10000, functional_features=functional_features,\n",
        "          nonFunctional_features=nonFunctional_features):\n",
        "    \n",
        "    losses = []\n",
        "    ids, generator, discriminator = clf, g, d\n",
        "    generator_in = generate_latent_points(70, batch_size)\n",
        "    \n",
        "    generator_optimizer = tf.keras.optimizers.RMSprop(1e-4)\n",
        "    discriminator_optimizer = tf.keras.optimizers.RMSprop(1e-2)\n",
        "    \n",
        "    for e in range(epochs):\n",
        "        for i in range(int(normal_data.shape[0]/batch_size)):\n",
        "            normal_samples = get_batch(normal_data, batch_size=batch_size)\n",
        "            malicious_samples = get_batch(malicious_data, batch_size=batch_size)\n",
        "                        \n",
        "            normal_samples = tf.convert_to_tensor(normal_samples, float)\n",
        "            malicious_samples = tf.convert_to_tensor(malicious_samples, float)\n",
        "            functional_features = tf.convert_to_tensor(functional_features, float)\n",
        "            nonFunctional_features = tf.convert_to_tensor(nonFunctional_features, float)\n",
        "            generator_in = tf.convert_to_tensor(generator_in, float)\n",
        "\n",
        "\n",
        "            \n",
        "            with tf.GradientTape() as gTape, tf.GradientTape() as dTape:\n",
        "                generated_maliciousSamples = tf.math.add(tf.math.multiply(malicious_samples, \n",
        "                                                                          functional_features),\n",
        "                                                    tf.math.multiply(generator(generator_in), \n",
        "                                                                     nonFunctional_features))\n",
        "            \n",
        "                ids_samples = tf.concat([generated_maliciousSamples, normal_samples], 0)\n",
        "#                 ids_samples = tf.Session().run(ids_samples)\n",
        "#                 ids_predictions = ids.predict(ids_samples)\n",
        "                ids_samples, ids_predictions, ids_loss = ids_compute(ids, ids_samples, batch_size=batch_size)\n",
        "                #print(\"ids loss:\", ids_loss)\n",
        "                     \n",
        "                d_predictions = discriminator(ids_samples)\n",
        "            \n",
        "                d_loss = discriminator_loss(ids_predictions, d_predictions)*100000\n",
        "                #print(\"dloss:\", d_loss)\n",
        "\n",
        "\n",
        "                disGenOut = discriminator(generated_maliciousSamples)\n",
        "            \n",
        "                g_loss = generator_loss(disGenOut)*1000000\n",
        "                #print(\"gloss:\", g_loss)\n",
        "    \n",
        "            gGradients = gTape.gradient(g_loss, generator.trainable_variables)\n",
        "            dGradients = dTape.gradient(d_loss, discriminator.trainable_variables)\n",
        "\n",
        "            generator_optimizer.apply_gradients(zip(gGradients, generator.trainable_variables))\n",
        "            discriminator_optimizer.apply_gradients(zip(dGradients, discriminator.trainable_variables))\n",
        "            losses.append((d_loss, g_loss))\n",
        "\n",
        "        print(\"Epoch:{:>3}/{} Discriminator Loss:{:>7.4f} Generator Loss:{:>7.4f} IDS Loss:{:>7.4f}\".format(\n",
        "            e+1, epochs, d_loss, g_loss, ids_loss))            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TamaBnd3nCxO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b083f978-2038-48da-8b89-fdc88e8d0ae8"
      },
      "source": [
        "train()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1/10 Discriminator Loss:17.3396 Generator Loss:39.2695 IDS Loss: 0.5038\n",
            "Epoch:  2/10 Discriminator Loss: 2.7797 Generator Loss: 0.1766 IDS Loss: 0.5041\n",
            "Epoch:  3/10 Discriminator Loss: 0.0178 Generator Loss: 0.0016 IDS Loss: 0.5047\n",
            "Epoch:  4/10 Discriminator Loss: 0.2519 Generator Loss: 0.0869 IDS Loss: 0.5050\n",
            "Epoch:  5/10 Discriminator Loss: 2.7789 Generator Loss: 0.0000 IDS Loss: 0.5040\n",
            "Epoch:  6/10 Discriminator Loss:20.4633 Generator Loss: 0.0000 IDS Loss: 0.5046\n",
            "Epoch:  7/10 Discriminator Loss: 9.4784 Generator Loss: 0.0000 IDS Loss: 0.5054\n",
            "Epoch:  8/10 Discriminator Loss: 0.2956 Generator Loss: 0.0000 IDS Loss: 0.5050\n",
            "Epoch:  9/10 Discriminator Loss: 0.0141 Generator Loss: 0.0000 IDS Loss: 0.5044\n",
            "Epoch: 10/10 Discriminator Loss: 5.9453 Generator Loss: 0.0000 IDS Loss: 0.5038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttXFmcZ827N9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}