{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDSGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1dgshOPJWRGAg2Dhr99T9BbzpWxWKU934",
      "authorship_tag": "ABX9TyMOz6bVzgcCZ2Xx0ndfqdK7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Poojithavb/CMPE258-Project/blob/master/IDSGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqFrcKU9l2n-",
        "colab_type": "code",
        "outputId": "b0e05d0b-27a8-499e-85f3-23fc6d471f63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVSStAKhfVzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") \n",
        "import sys\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib.pyplot import plot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT_GHH3EgKV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pwd = os.getcwd()\n",
        "path = pwd + '/drive/My Drive/MachineLearningCVE'\n",
        "dfs = []\n",
        "filenames = glob(path + \"/*.csv\")\n",
        "filenames.sort()\n",
        "for filename in filenames:\n",
        "    dfs.append(pd.read_csv(filename))\n",
        "    \n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "df['Flow Bytes/s'] = df['Flow Bytes/s'].astype(float)\n",
        "df[' Flow Packets/s'] = df[' Flow Packets/s'].astype(float)\n",
        "df = df.fillna(0)\n",
        "df = df.replace([np.inf, -np.inf], sys.maxsize)\n",
        "#print(\"Columns which have NaN or NULLS: \")\n",
        "#for col in df.columns:\n",
        "#    print('%s: %d' % (col, df[col].isna().sum()))\n",
        "substr = 'ï¿½'\n",
        "df[' Label'] = df[' Label'].str.replace(substr, '-')\n",
        "encoding = {\n",
        "        \" Label\": {\"BENIGN\": 0, \"FTP-Patator\": 1, \"SSH-Patator\": 1, \n",
        "                   \"DoS slowloris\": 1, \"DoS Slowhttptest\": 1, \"DoS Hulk\": 1, \n",
        "                   \"DoS GoldenEye\": 1, \"Heartbleed\": 1, \n",
        "                   \"Web Attack - Brute Force\" : 1, \"Web Attack - XSS\": 1,\n",
        "                   \"Web Attack - Sql Injection\": 1, \"Infiltration\" : 1, \n",
        "                   \"Bot\":1, \"DDoS\":1,\"PortScan\": 1}\n",
        "        }\n",
        "df.replace(encoding, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7IoAtWThJDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_data = df.drop(columns=[\n",
        "    \"Fwd Avg Bytes/Bulk\",              \n",
        "    \" Fwd Avg Packets/Bulk\",           \n",
        "    \" Fwd Avg Bulk Rate\",              \n",
        "    \" Bwd Avg Bytes/Bulk\",             \n",
        "    \" Bwd Avg Packets/Bulk\", \n",
        "    \"Bwd Avg Bulk Rate\", \n",
        "    \" Bwd PSH Flags\", \n",
        "    \" Bwd URG Flags\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maxQLtenVdyJ",
        "colab_type": "code",
        "outputId": "161b9254-fc14-4e1f-8cc7-2b20b506ae62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "full_data.columns"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([' Destination Port', ' Flow Duration', ' Total Fwd Packets',\n",
              "       ' Total Backward Packets', 'Total Length of Fwd Packets',\n",
              "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max',\n",
              "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean',\n",
              "       ' Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
              "       ' Bwd Packet Length Min', ' Bwd Packet Length Mean',\n",
              "       ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s',\n",
              "       ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min',\n",
              "       'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max',\n",
              "       ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std',\n",
              "       ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Fwd URG Flags',\n",
              "       ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s',\n",
              "       ' Bwd Packets/s', ' Min Packet Length', ' Max Packet Length',\n",
              "       ' Packet Length Mean', ' Packet Length Std', ' Packet Length Variance',\n",
              "       'FIN Flag Count', ' SYN Flag Count', ' RST Flag Count',\n",
              "       ' PSH Flag Count', ' ACK Flag Count', ' URG Flag Count',\n",
              "       ' CWE Flag Count', ' ECE Flag Count', ' Down/Up Ratio',\n",
              "       ' Average Packet Size', ' Avg Fwd Segment Size',\n",
              "       ' Avg Bwd Segment Size', ' Fwd Header Length.1', 'Subflow Fwd Packets',\n",
              "       ' Subflow Fwd Bytes', ' Subflow Bwd Packets', ' Subflow Bwd Bytes',\n",
              "       'Init_Win_bytes_forward', ' Init_Win_bytes_backward',\n",
              "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean',\n",
              "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
              "       ' Idle Max', ' Idle Min', ' Label'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSU4fQ0thM5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "functional_cols = ['Idle Mean',\n",
        " ' Idle Std',\n",
        " ' Idle Min',\n",
        " ' Idle Max',\n",
        " ' Avg Bwd Segment Size',\n",
        " ' Average Packet Size',\n",
        " ' URG Flag Count',\n",
        " 'FIN Flag Count',\n",
        " ' Packet Length Variance',\n",
        " ' Packet Length Std',\n",
        " ' Packet Length Mean',\n",
        " ' Max Packet Length',\n",
        " ' Min Packet Length',\n",
        " ' Fwd IAT Max',\n",
        " ' Fwd IAT Std',\n",
        " ' Fwd IAT Mean',\n",
        " 'Fwd IAT Total',\n",
        " ' Flow IAT Max',\n",
        " ' Flow IAT Mean',\n",
        " ' Flow IAT Std',\n",
        " ' Bwd Packet Length Std',\n",
        " 'Bwd Packet Length Max',\n",
        " ' Bwd Packet Length Min',\n",
        " ' Bwd Packet Length Mean',\n",
        " ' Flow Duration']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXLpyI41hSQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "functional_features = np.array([col in functional_cols for col in full_data.columns[:-1]], dtype=np.float64)\n",
        "nonFunctional_features = np.array([col not in functional_cols for col in full_data.columns[:-1]], dtype=np.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyRMfIcvpPNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def checker(arr):\n",
        "  count = 0\n",
        "  for i in arr:\n",
        "    if i != 1:\n",
        "      count += 1\n",
        "  return count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1ceZTpUhj0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _toarray(X, Y):\n",
        "    X = X.astype(float)\n",
        "    X = np.asarray(X)\n",
        "    Y = np.asarray(Y)\n",
        "    Y = Y.reshape(-1, 1)\n",
        "    #Y = to_categorical(Y, 2)\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X)\n",
        "    X = scaler.transform(X)\n",
        "    return X, Y\n",
        "    \n",
        "def _getData(df):\n",
        "    _all = df\n",
        "    _norm = df.loc[df[' Label'] == 0]\n",
        "    _mal = df.loc[df[' Label'] != 0]\n",
        "    #print(checker(_mal[' Label']))\n",
        "    l_all = _all[' Label'][:,None]\n",
        "    l_norm = _norm[' Label']\n",
        "    l_mal = _mal[' Label']\n",
        "    _all = _all.drop([' Label'], axis=1)\n",
        "    _mal = _mal.drop([' Label'], axis=1)\n",
        "    _norm = _norm.drop([' Label'], axis=1)\n",
        "    data_all, label_all = _toarray(_all, l_all)\n",
        "    data_mal, label_mal = _toarray(_mal, l_mal)\n",
        "    #print(checker(label_mal))\n",
        "    data_norm, label_norm = _toarray(_norm, l_norm)\n",
        "\n",
        "    return data_all, label_all, data_mal, label_mal, data_norm, label_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWX7yc2Eho_2",
        "colab_type": "code",
        "outputId": "3666001c-4605-4194-f518-fa45cc5bd942",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_res, y_res = rus.fit_resample(full_data.iloc[:,:-1], full_data.iloc[:,-1])\n",
        "print('Resampled dataset shape %s' % Counter(y_res))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resampled dataset shape Counter({0: 557646, 1: 557646})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EpM4iFNiDVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "dataframe = pd.DataFrame(X_res)\n",
        "dataframe[' Label'] = y_res \n",
        "dataframe = shuffle(dataframe).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNy_rE99iLtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_all, Y_all, malicious_data, Y_mal, normal_data, Y_norm = _getData(dataframe)\n",
        "n_data = normal_data[int(len(normal_data)/2):]\n",
        "m_data = np.vstack((malicious_data, malicious_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWT_7kfR9Nrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mal_norm = np.vstack((m_data, n_data)) \n",
        "labels = np.hstack((np.ones(len(m_data)), np.zeros(len(n_data))))\n",
        "labels = labels.reshape(-1, 1)\n",
        "data = np.hstack((mal_norm, labels))\n",
        "np.random.shuffle(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nai1YhciOOo",
        "colab_type": "code",
        "outputId": "d1ffab0a-2eee-4306-d0d0-5f3c0d33cb72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Malicious_data\",malicious_data.shape)\n",
        "print(\"Normal_data\",normal_data.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Malicious_data (557646, 70)\n",
            "Normal_data (557646, 70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeYXnc1n9OhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _getIDSdata(arr):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for i in range(0, len(arr)):\n",
        "        a = arr[i][:-1]\n",
        "        data.append(a)\n",
        "        \n",
        "        b = arr[i][-1:]\n",
        "        labels.append(b)\n",
        "        \n",
        "    X = np.asarray(data)\n",
        "    y = np.asarray(labels)\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QttOrbv59UmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = _getIDSdata(data)\n",
        "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7ON_F-yiXwW",
        "colab_type": "code",
        "outputId": "a17dabba-4492-41f7-dfdc-ec3edab4c124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
              "                     criterion='gini', max_depth=None, max_features='auto',\n",
              "                     max_leaf_nodes=None, max_samples=None,\n",
              "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                     min_samples_leaf=1, min_samples_split=2,\n",
              "                     min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                     n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
              "                     warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XZnmIeHiaDK",
        "colab_type": "code",
        "outputId": "522a147a-d4e8-4bf8-e9ec-d801acd1510b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mal_pred = clf.predict(malicious_data)\n",
        "print(\"F1 Score Malcious Data: \", f1_score(np.ones_like(mal_pred), mal_pred, average='micro'))\n",
        "print(\"MSE Malcious Data: \", mean_squared_error(np.ones_like(mal_pred), mal_pred))\n",
        "norm_pred = clf.predict(normal_data)\n",
        "print(\"F1 Score Normal Data: \", f1_score(np.zeros_like(norm_pred), norm_pred, average='micro'))\n",
        "print(\"MSE Normal Data: \", mean_squared_error(np.zeros_like(norm_pred), norm_pred))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 Score Malcious Data:  1.0\n",
            "MSE Malcious Data:  0.0\n",
            "F1 Score Normal Data:  1.0\n",
            "MSE Normal Data:  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feCUmxCgiqjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _generator(input=(70,)):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(32, input_shape=(70,)))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    #model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(64))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    #model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(128))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    #model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(256))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    #model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(np.prod(70,)))\n",
        "    #model.add(layers.Activation('sigmoid'))\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HybE1Jfiit6L",
        "colab_type": "code",
        "outputId": "20f019ac-8a58-48cd-b320-3d36dd31b8ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "g = _generator()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32)                2272      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 70)                17990     \n",
            "=================================================================\n",
            "Total params: 65,638\n",
            "Trainable params: 64,678\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCpW0IPCiweu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _discriminator():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(128, input_shape=(70, )))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(128))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(64))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.LeakyReLU(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(1))\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quU3VaJ5iyWN",
        "colab_type": "code",
        "outputId": "34b05dfc-9c12-47cb-971d-6f5d52bd1c55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "d = _discriminator()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 128)               9088      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 35,201\n",
            "Trainable params: 34,561\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvKhI5v7i1dU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(data, batch_size):\n",
        "    indices = np.random.randint(low=0, high=len(data), size=batch_size)\n",
        "    np.random.shuffle(indices)\n",
        "    batch_data = data[indices]\n",
        "    return batch_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD2Fn_Tvi3xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_latent_points(latent_dim, n):\n",
        "    x_input = np.random.uniform(0, 1, (latent_dim * n))\n",
        "    x_input = x_input.reshape(n, latent_dim)\n",
        "    return x_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EQRbJZsLCj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _getPredictedNormalAdversial(arr):\n",
        "  for i in range(0, len(arr)):\n",
        "    n = []\n",
        "    a = []\n",
        "    if arr[i][-1:] == 0:\n",
        "      n.append(arr[i][:-1])\n",
        "    if arr[i][-1:] == 1:\n",
        "      a.append(arr[i][:-1])\n",
        "\n",
        "    normal = np.asarray(n)\n",
        "    attack = np.asarray(a)\n",
        "\n",
        "    return normal, attack"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc-Dj45MjCR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ids_compute(ids_classifer, ids_samples, batch_size):\n",
        "    a = tf.make_tensor_proto(ids_samples)\n",
        "    ids_samples = tf.make_ndarray(a)\n",
        "    #print(ids_samples.shape)\n",
        "    ids_preds = ids_classifer.predict(ids_samples)    \n",
        "    ids_preds = ids_preds.reshape(-1, 1)\n",
        "    mse = mean_squared_error(np.hstack([np.ones(int(len(ids_preds)/2)), np.zeros(int(len(ids_preds)/2))]), ids_preds)\n",
        "    ids_stack = np.hstack((ids_samples, ids_preds))\n",
        "    #print(ids_stack.shape)\n",
        "    ids_normal, ids_attack = _getPredictedNormalAdversial(ids_stack)\n",
        "    #print(ids_normal.shape)\n",
        "    #print(ids_attack.shape)\n",
        "    ids_normal = tf.convert_to_tensor(ids_normal, float)\n",
        "    ids_attack = tf.convert_to_tensor(ids_attack, float)\n",
        "\n",
        "    return ids_normal, ids_attack, mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W_rnoltqPEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _setTheshold(arr):\n",
        "  a = tf.where(tf.less(arr, 0.0), 0, arr) \n",
        "  arr = tf.where(tf.greater(a, 1.0), 1, a)\n",
        "  \n",
        "  return arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9__Dvosi88a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "def generator_loss(d_generated):\n",
        "    return tf.math.reduce_mean(d_generated)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7xSzJvRi6Z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(d_normal, d_generated):\n",
        "    return tf.math.subtract((tf.math.reduce_mean(d_normal)), tf.math.reduce_mean(d_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-PC0OD8iQYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_loss(losses):\n",
        "    labels = ['Discriminator', 'Generator', 'IDS-Classifier']\n",
        "    losses = np.array(losses)    \n",
        "    \n",
        "    fig, ax = plt.subplots()\n",
        "    plt.plot(losses.T[0], label='Discriminator')\n",
        "    plt.plot(losses.T[1], label='Generator')\n",
        "    plt.plot(losses.T[2], label='IDS-Classifier')\n",
        "    plt.title(\"Training Losses\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ici-CryFjI5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs=50, batch_size=5000, functional_features=functional_features,\n",
        "          nonFunctional_features=nonFunctional_features):\n",
        "    \n",
        "    losses = []\n",
        "    ids, generator, discriminator = clf, g, d\n",
        "    noise = generate_latent_points(70, batch_size)\n",
        "\n",
        "    generator_optimizer = tf.keras.optimizers.RMSprop(1e-4) #change to \n",
        "    discriminator_optimizer = tf.keras.optimizers.RMSprop(1e-2) #change to \n",
        "    \n",
        "    for e in range(epochs):\n",
        "        for i in range(int(normal_data.shape[0]/batch_size)):\n",
        "            normal_samples = get_batch(normal_data, batch_size=batch_size)\n",
        "            malicious_samples = get_batch(malicious_data, batch_size=batch_size)\n",
        "            generator_in = malicious_samples + noise\n",
        "                        \n",
        "            normal_samples = tf.convert_to_tensor(normal_samples, float)\n",
        "            malicious_samples = tf.convert_to_tensor(malicious_samples, float)\n",
        "            functional_features = tf.convert_to_tensor(functional_features, float)\n",
        "            nonFunctional_features = tf.convert_to_tensor(nonFunctional_features, float)\n",
        "            generator_in = tf.convert_to_tensor(generator_in, float)\n",
        "\n",
        "\n",
        "            \n",
        "            with tf.GradientTape() as gTape, tf.GradientTape() as dTape:\n",
        "                generated_maliciousSamples = tf.math.add(tf.math.multiply(malicious_samples, \n",
        "                                                                          functional_features), \n",
        "                                                         _setTheshold(tf.math.multiply(generator(generator_in), \n",
        "                                                                     nonFunctional_features)))\n",
        "                \n",
        "                #print(generated_maliciousSamples)\n",
        "            \n",
        "                ids_samples = tf.concat([generated_maliciousSamples, normal_samples], 0)\n",
        "                #print(ids_samples.shape)\n",
        "#                 ids_samples = tf.Session().run(ids_samples)\n",
        "#                 ids_predictions = ids.predict(ids_samples)\n",
        "                ids_predictedNormal, ids_predictedAttacks, ids_loss = ids_compute(ids, ids_samples, batch_size=batch_size)\n",
        "                #print(\"ids loss:\", ids_loss)\n",
        "\n",
        "                disGenOut = discriminator(generated_maliciousSamples)\n",
        "                g_loss = generator_loss(disGenOut)\n",
        "                #print(\"gloss:\", g_loss)\n",
        "                     \n",
        "                d_normal = discriminator(ids_predictedNormal)\n",
        "                d_generated = discriminator(ids_predictedAttacks)\n",
        "                d_loss = discriminator_loss(d_normal, d_generated)*-1\n",
        "                #print(\"dloss:\", d_loss)\n",
        "    \n",
        "            gGradients = gTape.gradient(g_loss, generator.trainable_variables)\n",
        "            dGradients = dTape.gradient(d_loss, discriminator.trainable_variables)\n",
        "\n",
        "            generator_optimizer.apply_gradients(zip(gGradients, generator.trainable_variables))\n",
        "            discriminator_optimizer.apply_gradients(zip(dGradients, discriminator.trainable_variables))\n",
        "            losses.append((d_loss, g_loss, ids_loss))\n",
        "\n",
        "        print(\"Epoch:{:>3}/{} Discriminator Loss:{:>7.4f} Generator Loss:{:>7.4f} IDS Loss:{:>7.4f}\".format(\n",
        "            e+1, epochs, d_loss, g_loss, ids_loss)) \n",
        "    \n",
        "    plot_loss(losses)\n",
        "\n",
        "    return generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TamaBnd3nCxO",
        "colab_type": "code",
        "outputId": "33322927-9873-49d6-eac3-8fb58689387b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 70) for input Tensor(\"dense_5_input:0\", shape=(None, 70), dtype=float32), but it was called on an input with incompatible shape (0,).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-5e2a2b840bd9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size, functional_features, nonFunctional_features)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m#print(\"gloss:\", g_loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0md_normal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_predictedNormal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0md_generated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_predictedAttacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_normal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_generated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    275\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    717\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    718\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_tensor_dense_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5575\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5576\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5577\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5578\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5579\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: In[0] is not a matrix. Instead it has shape [0] [Op:MatMul]"
          ]
        }
      ]
    }
  ]
}